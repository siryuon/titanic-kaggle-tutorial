{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Titanic_Tutorial.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPBjqdOcCOavGNzlEG3E3XB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"dyEoNO7ZYV13","executionInfo":{"status":"ok","timestamp":1605444696310,"user_tz":-540,"elapsed":2274,"user":{"displayName":"‍조문기[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"","userId":"11238996365821313784"}},"outputId":"b76234c8-296a-4fc5-b440-37aac83af46a","colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torchvision.transforms import functional as F\n","import random\n","import tensorflow as tf\n","from sklearn.preprocessing import StandardScaler\n","import time\n","from tqdm import tqdm\n","from sklearn.model_selection import StratifiedKFold\n","\n","batch_size = 32\n","train_epochs = 6\n","\n","train = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/data/titanic.csv')\n","test = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/data/titanic_test.csv')\n","target = train[\"Survived\"]\n","\n","train = pd.concat([train, test], sort=True)\n","\n","def get_text_features(train):\n","    train['Length_Name'] = train['Name'].astype(str).map(len)\n","    return train\n","\n","train = get_text_features(train)\n","\n","cols = ['Cabin','Embarked','Name','Sex','Ticket',]\n","\n","num_cols = list(set(train.columns) - set(cols) - set([\"Survived\"]))\n","\n","def encode(encoder, x):\n","    len_encoder = len(encoder)\n","    try:\n","        id = encoder[x]\n","    except KeyError:\n","        id = len_encoder\n","    return id\n","\n","encoders = [{} for col in cols]\n","\n","\n","for i, col in enumerate(cols):\n","    print('encoding %s ...' % col, end=' ')\n","    encoders[i] = {l: id for id, l in enumerate(train.loc[:, col].astype(str).unique())}\n","    train[col] = train[col].astype(str).apply(lambda x: encode(encoders[i], x))\n","    print('Done')\n","\n","embed_sizes = [len(encoder) for encoder in encoders]\n","\n","train[num_cols] = train[num_cols].fillna(0)\n","\n","scaler = StandardScaler()\n","train[num_cols] = scaler.fit_transform(train[num_cols])\n","\n","class Model(nn.Module):\n","    def __init__(self, in_features, out_features, bias=True, p=0.5):\n","        super().__init__()\n","        self.linear = nn.Linear(in_features, out_features, bias)\n","        self.relu = nn.ReLU()\n","        self.drop = nn.Dropout(p)\n","        \n","    def forward(self, x):\n","        x = self.linear(x)\n","        x = self.relu(x)\n","        x = self.drop(x)\n","        return x\n","\n","net = nn.Sequential(Model(12, 32), nn.Linear(32, 1)) \n","\n","X_train = train.loc[np.isfinite(train.Survived), :]\n","X_train = X_train.drop([\"Survived\"], axis=1).values\n","Y_train = target.values\n","\n","X_test = train.loc[~np.isfinite(train.Survived), :]\n","\n","X_test = X_test.drop([\"Survived\"], axis=1).values      \n","splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=None).split(X_train, Y_train))\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","train_preds = np.zeros((len(X_train)))\n","test_preds = np.zeros((len(X_test)))\n","\n","\n","x_test_cuda = torch.tensor(X_test, dtype=torch.float32)\n","test = torch.utils.data.TensorDataset(x_test_cuda)\n","test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n","\n","for i, (train_idx, valid_idx) in enumerate(splits):\n","    x_train_fold = torch.tensor(X_train[train_idx], dtype=torch.float32)\n","    y_train_fold = torch.tensor(Y_train[train_idx, np.newaxis], dtype=torch.float32)\n","    x_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.float32)\n","    y_val_fold = torch.tensor(Y_train[valid_idx, np.newaxis], dtype=torch.float32)\n","    \n","    model = net    \n","    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n","    optimizer = torch.optim.Adam(model.parameters())\n","    \n","    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n","    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n","    \n","    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n","    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n","    \n","    print(f'Fold {i + 1}')\n","    \n","    for epoch in range(train_epochs):\n","        \n","        model.train()\n","        avg_loss = 0.\n","        for x_batch, y_batch in tqdm(train_loader, disable=True):\n","            y_pred = model(x_batch)\n","            loss = loss_fn(y_pred, y_batch)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            avg_loss += loss.item() / len(train_loader)\n","        \n","        model.eval()\n","        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n","        test_preds_fold = np.zeros(len(X_test))\n","        avg_val_loss = 0.\n","        for i, (x_batch, y_batch) in enumerate(valid_loader):\n","            y_pred = model(x_batch).detach()\n","            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n","            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n","        \n","        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}'.format(epoch + 1, train_epochs, avg_loss, avg_val_loss))\n","        \n","    for i, (x_batch,) in enumerate(test_loader):\n","        y_pred = model(x_batch).detach()\n","\n","        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n","\n","    train_preds[valid_idx] = valid_preds_fold\n","    test_preds += test_preds_fold / len(splits)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["encoding Cabin ... Done\n","encoding Embarked ... Done\n","encoding Name ... Done\n","encoding Sex ... Done\n","encoding Ticket ... Done\n","Fold 1\n","Epoch 1/6 \t loss=1678.7789 \t val_loss=1025.9364\n","Epoch 2/6 \t loss=792.5968 \t val_loss=135.7170\n","Epoch 3/6 \t loss=550.8131 \t val_loss=168.0948\n","Epoch 4/6 \t loss=424.7154 \t val_loss=135.4763\n","Epoch 5/6 \t loss=381.3714 \t val_loss=81.1264\n","Epoch 6/6 \t loss=325.0955 \t val_loss=65.8386\n","Fold 2\n","Epoch 1/6 \t loss=266.9927 \t val_loss=48.1998\n","Epoch 2/6 \t loss=211.0362 \t val_loss=54.1199\n","Epoch 3/6 \t loss=154.1299 \t val_loss=25.9360\n","Epoch 4/6 \t loss=112.5927 \t val_loss=28.7123\n","Epoch 5/6 \t loss=73.1350 \t val_loss=25.0544\n","Epoch 6/6 \t loss=57.5428 \t val_loss=22.4604\n","Fold 3\n","Epoch 1/6 \t loss=41.1001 \t val_loss=21.8561\n","Epoch 2/6 \t loss=28.4941 \t val_loss=22.5243\n","Epoch 3/6 \t loss=24.1932 \t val_loss=20.8069\n","Epoch 4/6 \t loss=23.3517 \t val_loss=18.7704\n","Epoch 5/6 \t loss=22.3143 \t val_loss=17.8407\n","Epoch 6/6 \t loss=21.6347 \t val_loss=19.4776\n","Fold 4\n","Epoch 1/6 \t loss=21.8632 \t val_loss=17.9547\n","Epoch 2/6 \t loss=20.2935 \t val_loss=18.1822\n","Epoch 3/6 \t loss=20.5947 \t val_loss=18.0752\n","Epoch 4/6 \t loss=19.8766 \t val_loss=18.3399\n","Epoch 5/6 \t loss=19.2902 \t val_loss=18.0280\n","Epoch 6/6 \t loss=19.4134 \t val_loss=18.1172\n","Fold 5\n","Epoch 1/6 \t loss=19.7742 \t val_loss=18.3899\n","Epoch 2/6 \t loss=19.2487 \t val_loss=19.7018\n","Epoch 3/6 \t loss=18.9526 \t val_loss=18.3510\n","Epoch 4/6 \t loss=18.7500 \t val_loss=18.1672\n","Epoch 5/6 \t loss=18.7901 \t val_loss=18.1443\n","Epoch 6/6 \t loss=18.9892 \t val_loss=18.2943\n"],"name":"stdout"}]}]}